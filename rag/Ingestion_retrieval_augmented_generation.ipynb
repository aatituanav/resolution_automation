{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8uK9UQZsMOt"
   },
   "source": [
    "# INGESTION\n",
    "Este proceso se engarga de la ingesta de un sistema RAG o generación aumentada por recuperación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 langchain faiss-cpu -q > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae todo el texto de un archivo PDF y lo devuelve como una cadena concatenada.\n",
    "    Parameters:\n",
    "        pdf_path (str): Ruta completa al archivo PDF del cual se desea extraer el texto.\n",
    "    Returns:\n",
    "        str: Cadena de texto que contiene todo el contenido textual del PDF, con cada\n",
    "             página separada por un salto de línea.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"ley_educacion.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_legal_text(text):\n",
    "    \"\"\"\n",
    "    Limpia texto eliminando pies de página e información de paginación específicos.\n",
    "    Parameters:\n",
    "        text (str): Texto legal crudo extraído de un documento PDF\n",
    "    Returns:\n",
    "        str: Texto limpio sin la información de pies de página y paginación\n",
    "    \"\"\"\n",
    "    footer_pattern = r'LEY ORGÁNICA DE EDUCACIÓN SUPERIOR, LOES\\s*-\\s*Página\\s*\\d+'\n",
    "    finder_pattern = r'FINDER LOYAL\\s*-\\s*www\\.lexis\\.com\\.ec?'\n",
    "    combined_pattern = r'(?:' + footer_pattern + r'[\\s\\r\\n]*' + finder_pattern + r'|' + finder_pattern + r'[\\s\\r\\n]*' + footer_pattern + r')'\n",
    "    cleaned_text = re.sub(combined_pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText = clean_legal_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_articles(text):\n",
    "  \"\"\"\n",
    "  Separa un texto legal en artículos individuales, excluyendo disposiciones generales,\n",
    "  transitorias y finales.\n",
    "  Parametro:\n",
    "      text (str): Texto legal completo que contiene artículos numerados\n",
    "  Return:\n",
    "      list: Lista de strings donde cada elemento es un artículo completo con su contenido\n",
    "  \"\"\"\n",
    "  secciones = re.split(r'(?=DISPOSICIONES GENERALES\\n|DISPOSICIONES TRANSITORIAS\\n|DISPOSICIONES FINALES\\n)',\n",
    "                  text, flags=re.IGNORECASE)\n",
    "  partes = re.split(r'(Art\\. \\d+(?:\\.\\d+)*\\.-)', secciones[0])\n",
    "  articulos = []\n",
    "  for i in range(1, len(partes), 2):\n",
    "      if i + 1 < len(partes):\n",
    "          articulo_completo = partes[i] + partes[i+1]\n",
    "          articulos.append(articulo_completo.strip())\n",
    "  return articulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_legal_text(text, chunk_size=1024, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Crea chunks, preservando la estructura de artículos.\n",
    "    Parametros:\n",
    "        text (str): Texto legal completo a dividir\n",
    "        chunk_size (int): Tamaño máximo de cada fragmento en caracteres\n",
    "        chunk_overlap (int): Solapamiento entre fragmentos consecutivos en caracteres\n",
    "    Returns:\n",
    "        list: Lista de fragmentos de texto, donde cada artículo se mantiene intacto si es pequeño\n",
    "              o se divide en chunks superpuestos si es demasiado grande\n",
    "    \"\"\"\n",
    "    articulos = separate_articles(text)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    chunks_finales = []\n",
    "    for articulo in articulos:\n",
    "        if len(articulo) > chunk_size:\n",
    "            # Divide el artículo en chunks pequeños\n",
    "            chunks = splitter.split_text(articulo)\n",
    "            chunks_finales.extend(chunks)\n",
    "        else:\n",
    "            chunks_finales.append(articulo)\n",
    "    return chunks_finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividiendo en chunks...\n"
     ]
    }
   ],
   "source": [
    "print(\"Dividiendo en chunks...\")\n",
    "chunk_size = 1024\n",
    "chunk_overlap=150\n",
    "chunks = chunk_legal_text(cleanText, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNK 1 ===\n",
      "Art. 1.-Ámbito.-Esta Ley regula el sistema de educación superior en el país, a los organismos e\n",
      "instituciones que lo integran; determina derechos, deberes y obligaciones de las personas naturales\n",
      "y jurídicas, y establece las respectivas sanciones por el incumplimiento de las disposiciones\n",
      "contenidas en la Constitución y la presente Ley.\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== CHUNK 2 ===\n",
      "Art. 2.-Objeto.-Esta Ley tiene como objeto definir sus principios, garantizar el derecho a la educación\n",
      "superior de calidad que propenda a la excelencia interculturalidad, al acceso universal, permanencia,\n",
      "movilidad y egreso sin discriminación alguna y con gratuidad en el ámbito público hasta el tercer\n",
      "nivel.\n",
      "Nota: Artículo reformado por artículo 2 de Ley No. 0, publicada en Registro Oficial Suplemento 297\n",
      "de 2 de Agosto del 2018 .\n",
      "Concordancias:\n",
      "CONSTITUCIÓN DE LA REPÚBLICA DEL ECUADOR, Arts. 11, 346\n",
      "CAPÍTULO 2\n",
      "FINES DE LA EDUCACIÓN SUPERIOR\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== CHUNK 3 ===\n",
      "Art. 3.-Fines de la Educación Superior.-La educación superior de carácter humanista, intercultural y\n",
      "científica constituye un derecho de las personas y un bien público social que, de conformidad con la\n",
      "Constitución de la República, responderá al interés público y no estará al servicio de intereses\n",
      "individuales y corporativos.\n",
      "Nota: Artículo reformado por artículo 3 de Ley No. 0, publicada en Registro Oficial Suplemento 297\n",
      "de 2 de Agosto del 2018 .\n",
      "Concordancias:\n",
      "CONSTITUCIÓN DE LA REPÚBLICA DEL ECUADOR, Arts. 27, 28, 350\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== CHUNK 4 ===\n",
      "Art. 4.-Derecho a la Educación Superior.-El derecho a la educación superior consiste en el ejercicio\n",
      "efectivo de la igualdad de oportunidades, en función de los méritos respectivos, a fin de acceder a\n",
      "una formación académica y profesional con producción de conocimiento pertinente y de excelencia.\n",
      "Las ciudadanas y los ciudadanos en forma individual y colectiva, las comunidades, pueblos y\n",
      "nacionalidades tienen el derecho y la responsabilidad de participar en el proceso educativo superior,\n",
      "a través de los mecanismos establecidos en la Constitución y esta Ley.\n",
      "Concordancias:\n",
      "CONSTITUCIÓN DE LA REPÚBLICA DEL ECUADOR, Arts. 26, 57\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== CHUNK 5 ===\n",
      "Art. 5.-Derechos de las y los estudiantes.-Son derechos de las y los estudiantes los siguientes:\n",
      "a) Acceder, movilizarse, permanecer, egresar y titularse sin discriminación conforme sus méritos\n",
      "académicos;\n",
      "b) Acceder a una educación superior de calidad y pertinente, que permita iniciar una carrera\n",
      "académica y/o profesional en igualdad de oportunidades;\n",
      "c) Contar y acceder a los medios y recursos adecuados para su formación superior; garantizados por\n",
      "la Constitución;\n",
      "d) Participar en el proceso de evaluación y acreditación de su carrera;\n",
      "e) Elegir y ser elegido para las representaciones estudiantiles e integrar el cogobierno, en el caso de\n",
      "las universidades y escuelas politécnicas;\n",
      "f) Ejercer la libertad de asociarse, expresarse y completar su formación bajo la más amplia libertad\n",
      "de cátedra e investigativa;\n",
      "g) Participar en el proceso de construcción, difusión y aplicación del conocimiento;\n",
      "h) El derecho a recibir una educación superior laica, intercultural, democrática, incluyente y diversa,\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar resultados\n",
    "for i, articulo in enumerate(chunks[:5], 1):\n",
    "    print(f\"=== CHUNK {i} ===\")\n",
    "    print(articulo)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Función para generar embeddings con Jina V3\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Genera embeddings vectoriales usando el modelo Jina Embeddings V3.\n",
    "    Parámetros:\n",
    "        chunks (list): Lista de chunks a convertir en embeddings\n",
    "    Retorna:\n",
    "        tuple: Tupla que contiene:\n",
    "            - embeddings: Array con los vectores de embeddings generados\n",
    "            - model: Instancia del modelo SentenceTransformer utilizado\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(\n",
    "        \"jinaai/jina-embeddings-v3\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        chunks,\n",
    "        task=\"retrieval.passage\",  # Para chunks de documentos\n",
    "        show_progress_bar=True,\n",
    "        batch_size=32\n",
    "    )\n",
    "    return embeddings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f6c4f5bc81486e9854366e22736af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/378 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caf862905b543fdbeee608bc3283760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d496dcd75b445db2194e2df8203c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a160e8a897be439692c0450bb3d33b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "custom_st.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-embeddings-v3:\n",
      "- custom_st.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de8855c945c45279e912046c0c1a852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af64c1ccf3c640ae91bcd034a9763e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_xlm_roberta.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- configuration_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1eca1e72016425bb7f41933f2f5574e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_lora.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f5b232b684090bc4f28d28a040225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_xlm_roberta.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c6a0b04cad413cad929edd1330cfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mlp.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- mlp.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b6af860fbd49fa8b3fa720d18555a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2edacdbab84ecf93b93e18cb372868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "xlm_padding.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- xlm_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355439887a5149ecb85fb9b95b8b0f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rotary.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- rotary.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7877668308d84a7586deae75b0f3385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "block.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dddd1c050b544d6917b4bc09e61dee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stochastic_depth.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- stochastic_depth.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca8effb453b4e46a193cd44b3fcb355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mha.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- block.py\n",
      "- stochastic_depth.py\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- modeling_xlm_roberta.py\n",
      "- mlp.py\n",
      "- embedding.py\n",
      "- xlm_padding.py\n",
      "- rotary.py\n",
      "- block.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- modeling_lora.py\n",
      "- modeling_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3c95ee51fc4a0daae39831a1672747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4486c574924462b91a148e371d9567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55240ee00c34d42b1d5bb9b84c206a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854a8260f73e42078630ad55bc11537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f255c4f4114f2abf6ecdbf1110d724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328c8b433e6b4ba2ab6be049d49aec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generar embeddings\n",
    "print(\"Generando embeddings...\")\n",
    "embeddings, model = generate_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Creación del índice FAISS\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Crea un índice FAISS para almacenar y buscar embeddings de manera eficiente.\n",
    "    Parámetros:\n",
    "        embeddings: Array numpy con los vectores de embeddings a indexar\n",
    "    Retorna:\n",
    "        index: Índice FAISS configurado con los embeddings proporcionados\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando índice FAISS...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creando índice FAISS...\")\n",
    "index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modifica la función de guardado para almacenamiento local\n",
    "def save_locally(chunks, embeddings, index, model, base_path=\".\"):\n",
    "    \"\"\"Guarda la base de datos RAG localmente\"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    # Guardar chunks\n",
    "    with open(f\"{base_path}/chunks.pkl\", \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    # Guardar embeddings\n",
    "    np.save(f\"{base_path}/embeddings.npy\", embeddings)\n",
    "\n",
    "    faiss.write_index(index, f\"{base_path}/faiss_index.bin\")\n",
    "\n",
    "    metadata = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"embedding_dimension\": embeddings.shape[1],\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"model_name\": \"jinaai/jina-embeddings-v3\"\n",
    "    }\n",
    "\n",
    "    with open(f\"{base_path}/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False)\n",
    "    print(f\"Base de datos guardada en: {base_path}/\")\n",
    "\n",
    "# Función para cargar localmente\n",
    "def load_local(base_path=\"rag_database\"):\n",
    "    \"\"\"Carga la base de datos RAG desde almacenamiento local\"\"\"\n",
    "\n",
    "    with open(f\"{base_path}/chunks.pkl\", \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    embeddings = np.load(f\"{base_path}/embeddings.npy\")\n",
    "\n",
    "    index = faiss.read_index(f\"{base_path}/faiss_index.bin\")\n",
    "\n",
    "    with open(f\"{base_path}/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "    return chunks, embeddings, index, model, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando localmente...\n",
      "Base de datos guardada en: ./\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando localmente...\")\n",
    "save_locally(chunks, embeddings, index, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "#CARGAR DATOS\n",
    "chunks, embeddings, index, model, metadata = load_local(file_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
